{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Minor Project 3.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_H-7gPuEOGkC",
        "8EXbdUQXqG3y",
        "nKZkfJN3ONxV",
        "BBJW-HewOUP_",
        "UlBHjVyJOYTQ",
        "Y4NBFe0_jNye",
        "xH20hlmQjVy9",
        "zyQOPLpZe1da",
        "oV2GylL4jY5l",
        "hU5SXpVbNL05",
        "aeUsV-UAdl0x",
        "4gcrGxYKkRUI",
        "IxpbmgDG-qaW",
        "A2F6bwQsaQ49",
        "L73Vx1kT-z0P",
        "h_oFFTA6AECx",
        "kFZrZzGesNxg",
        "Gd8pk-UvsQ8z",
        "eK9rsoSNAPqG",
        "0gNbF-tLXFNH",
        "gZPt3so_XRy_",
        "JEF4XNWLXXzz",
        "kUz3iTW6Xalp"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSGuyMStPqIN"
      },
      "source": [
        "# Accident Detection System"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gr5ehALHjj4h"
      },
      "source": [
        "# Importing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msmxXEW1vCNE",
        "outputId": "24464659-aa71-404f-bb6b-2241619ebf46"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUgc1a0A4OBP"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import cv2 \n",
        "import os\n",
        "import glob\n",
        "\n",
        "from skimage import transform\n",
        "import skimage\n",
        "\n",
        "from sklearn.model_selection import train_test_split \n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from keras.preprocessing.image import load_img\n",
        "\n",
        "from keras.layers import LSTM\n",
        "from keras.models import Model \n",
        "from keras.layers import Input,Dense,TimeDistributed\n",
        "\n",
        "from keras.layers import Dropout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H-7gPuEOGkC"
      },
      "source": [
        "# Making Frames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EXbdUQXqG3y"
      },
      "source": [
        "## Data Distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZag7NyhrLpm"
      },
      "source": [
        "![Data Distribution Dark.PNG](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKZkfJN3ONxV"
      },
      "source": [
        "## Positive Cases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2Te1jLPzX7k"
      },
      "source": [
        "os.chdir('/content/drive/MyDrive/Minor Project 3/CADP/Videos/Positive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dp0eipbyGQpb"
      },
      "source": [
        "def FrameCapture(path,folder): \n",
        "  cap = cv2.VideoCapture(path)\n",
        "  property_id = int(cv2.CAP_PROP_FRAME_COUNT) \n",
        "  length = int(cv2.VideoCapture.get(cap, property_id))\n",
        "  count = 0\n",
        "  success = 1\n",
        "  \n",
        "  if length > 100:\n",
        "    cut = length-99\n",
        "  else:\n",
        "    cut = 0\n",
        "\n",
        "  dir =  \"/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/\"+folder\n",
        "  os.mkdir(dir)\n",
        " \n",
        "\n",
        "  for i in range(length):\n",
        "    success, image = cap.read()\n",
        "    if count >= cut :\n",
        "      n = count-cut    \n",
        "      cv2.imwrite(dir+'/'+str(n)+'.jpg', image)\n",
        "    count += 1\n",
        "\n",
        "\n",
        "temp_dir = os.getcwd()\n",
        "\n",
        "for video_file in os.listdir(temp_dir):\n",
        "  path = temp_dir+\"/\"+video_file\n",
        "  FrameCapture(path,video_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBJW-HewOUP_"
      },
      "source": [
        "## Negative Cases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qdo-d8LvcMm-"
      },
      "source": [
        "os.chdir('/content/drive/MyDrive/Minor Project 3/CADP/Videos/Negative')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsRRn37HOXvt"
      },
      "source": [
        "def FrameCapture(path,folder): \n",
        "  cap = cv2.VideoCapture(path)\n",
        "  property_id = int(cv2.CAP_PROP_FRAME_COUNT) \n",
        "  length = int(cv2.VideoCapture.get(cap, property_id))\n",
        "  count = 0\n",
        "  success = 1\n",
        "  \n",
        "  if length > 100:\n",
        "    cut = length-99\n",
        "  else:\n",
        "    cut = 0\n",
        "\n",
        "  #dir = os.path.join(os.getcwd(),folder)\n",
        "  dir =  \"/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/\"+folder\n",
        "  os.mkdir(dir)\n",
        " \n",
        "  #print(folder,cut,sep=' ')\n",
        "  #while success:\n",
        "  for i in range(length):\n",
        "    success, image = cap.read()\n",
        "    if count >= cut :\n",
        "      n = count-cut\n",
        "      #cv2.imwrite(dir+\"/frame%d.jpg\" % n, image) \n",
        "      cv2.imwrite(dir+'/'+str(n)+'.jpg', image)\n",
        "    count += 1\n",
        "\n",
        "\n",
        "temp_dir = os.getcwd()\n",
        "\n",
        "for video_file in os.listdir(temp_dir):\n",
        "  path = temp_dir+\"/\"+video_file\n",
        "  FrameCapture(path,video_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlBHjVyJOYTQ"
      },
      "source": [
        "# More Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4NBFe0_jNye"
      },
      "source": [
        "## Storing Positive and Negative Frames in diff variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KC183uDg_D-",
        "outputId": "1cea1730-7cb6-4ff5-c719-3f80532ad604"
      },
      "source": [
        "os.chdir('/content/drive/MyDrive/Minor Project 3/CADP/Frames')\n",
        "\n",
        "img_filepath = os.getcwd()                                      \n",
        "neg = glob.glob(img_filepath + '/Negative/*.mp4')           \n",
        "pos = glob.glob(img_filepath + '/Positive/*.mp4')             \n",
        "\n",
        "all_files = np.concatenate((pos, neg))\n",
        "print(len(neg), len(pos))                   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "140 116\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJUUoSvtp-Zp",
        "outputId": "779f189d-3aa8-41de-9c16-99c0168e8ed8"
      },
      "source": [
        "len(all_files)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xH20hlmQjVy9"
      },
      "source": [
        "## Labelling Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADkw1Pm7sRK4",
        "outputId": "4b61f8b7-ba8f-44c2-caf0-d58ddbbbff7f"
      },
      "source": [
        "#Labelling positive cases\n",
        "\n",
        "pos_label = [[0, 1]]*len(pos)\n",
        "len(pos_label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "116"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4azc78SAtsQh",
        "outputId": "6e0e47d4-5c73-4c31-d1f0-5d60f5e198c5"
      },
      "source": [
        "#Labelling negative cases\n",
        "\n",
        "neg_label = [[1, 0]]*len(neg)\n",
        "len(neg_label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "140"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlWZ1H-IqQHr",
        "outputId": "0c1d9178-1df8-442d-9b8a-15ea6a349f8f"
      },
      "source": [
        "# Storing both in a variable\n",
        "\n",
        "labels = pos_label + neg_label\n",
        "len(labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38bKrd62qt3N"
      },
      "source": [
        "# Useless Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_ZPDnEdenRQ"
      },
      "source": [
        "## Function to Resize and convert Frames to Grayscale"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cW6d1Ego5kzi"
      },
      "source": [
        "def load_set(videofile):\n",
        "    #vidcap = cv2.VideoCapture(videofile)\n",
        "    error = ''    \n",
        "    success = True\n",
        "    flag = 0\n",
        "    #while success:\n",
        "        #success, img = vidcap.read()  \n",
        "\n",
        "    frames = [] \n",
        "    for j in range(99):\n",
        "        flag = 0\n",
        "        try:\n",
        "          img = cv2.imread(videofile + '/'+str(j) + '.jpg')\n",
        "          #success, img = vidcap.read()\n",
        "          flag += 1\n",
        "\n",
        "          #print(np.array(img).shape)\n",
        "\n",
        "          tmp = skimage.color.rgb2gray(np.array(img))\n",
        "          flag += 1\n",
        "          tmp = transform.resize(tmp, (144, 256))\n",
        "          flag += 1\n",
        "          \n",
        "          frames.append(tmp)     \n",
        "        except:\n",
        "          print(flag, end = ' ')\n",
        "          pass \n",
        "\n",
        "        #if np.shape(frames[0])!=(144,256):\n",
        "            #error = 'Video is not the correct resolution.'\n",
        "    #vidcap.release()\n",
        "    return frames, error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKFOooV1NaxP"
      },
      "source": [
        "from PIL import Image\n",
        "\n",
        "videofile = '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/55.mp4'\n",
        "\n",
        "main_path = videofile\n",
        "\n",
        "list_images_path = list(sorted(os.listdir(main_path)))\n",
        "\n",
        "temp = []\n",
        "list_images_path\n",
        "\n",
        "for path in list_images_path:\n",
        "  img1 = cv2.imread(os.path.join(main_path, path))\n",
        "  img1.shape\n",
        "  img1.resize(144,256)\n",
        "  #img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
        "  temp.append(img1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxhEnjHkNapW"
      },
      "source": [
        "def load_set(videofile):\n",
        "    #vidcap = cv2.VideoCapture(videofile)\n",
        "    error = ''    \n",
        "    success = True\n",
        "    flag = 0\n",
        "    #while success:\n",
        "        #success, img = vidcap.read()  \n",
        "\n",
        "    frames = [] \n",
        "    for j in range(99):\n",
        "        flag = 0\n",
        "        try:\n",
        "          img = cv2.imread(videofile + '/'+str(j) + '.jpg')\n",
        "          #success, img = vidcap.read()\n",
        "          flag += 1\n",
        "\n",
        "          #print(np.array(img).shape)\n",
        "\n",
        "          tmp = skimage.color.rgb2gray(np.array(img))\n",
        "          flag += 1\n",
        "          tmp = transform.resize(tmp, (144, 256))\n",
        "          flag += 1\n",
        "          \n",
        "          frames.append(tmp)     \n",
        "        except:\n",
        "          print(flag, end = ' ')\n",
        "          pass \n",
        "\n",
        "        #if np.shape(frames[0])!=(144,256):\n",
        "            #error = 'Video is not the correct resolution.'\n",
        "    #vidcap.release()\n",
        "    return frames, error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyQOPLpZe1da"
      },
      "source": [
        "## Function to load horizontally flipped frames "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxakBhkk50Vb"
      },
      "source": [
        "def hori_flipped_load_set(videofile):\n",
        "  \n",
        "  vidcap = cv2.VideoCapture(videofile)\n",
        "  error = ''\n",
        "  success = True\n",
        "  \n",
        "  while success:\n",
        "    success, img = vidcap.read() \n",
        "    frames = []  \n",
        "    for j in range(99):\n",
        "        try:\n",
        "          success, img = vidcap.read()\n",
        "\n",
        "          tmp = skimage.color.rgb2gray(np.array(img))\n",
        "          tmp = skimage.transform.resize(tmp, (144, 256))\n",
        "          tmp = np.array(tmp)\n",
        "          tmp = np.flip(tmp, axis = 1)\n",
        "\n",
        "          frames.append(tmp)\n",
        "\n",
        "        except:\n",
        "          pass\n",
        "    if np.shape(frames[0])!=(144,256):\n",
        "      error = 'Video is not the correct resolution.'\n",
        "  vidcap.release()\n",
        "  return frames, error\t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oV2GylL4jY5l"
      },
      "source": [
        "## Make dataset function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-wtTQYvjZO_"
      },
      "source": [
        "def make_dataset(rand):\n",
        "  seq1 = np.zeros( (2*len(rand), 99, 144, 256)) \n",
        "  for i,fi in enumerate(rand):                   \n",
        "    print((i, fi))                           \n",
        "    if fi[-4:] == '.mp4' and i%2==0:\t\t\n",
        "      t = load_set(fi)             \n",
        "\n",
        "    elif fi[-4:] == '.mp4' and i%2==1:\n",
        "      t = hori_flipped_load_set(fi)    \n",
        "       \n",
        "    #if t.shape==(99,144,256):\n",
        "    if(len(t) == 99 and len(t[0]) == 144 and len(t[0][0]) == 256):                 \n",
        "      seq1[i] = t                            \n",
        "    else:# TypeError:\n",
        "      print(\"Error\")\n",
        "      pass                              \n",
        "  print((seq1.shape))\n",
        "  return seq1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LJj6RDb5LWY_",
        "outputId": "a5f4d5f4-8c82-42af-db2f-aeb5bb9e6e50"
      },
      "source": [
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Minor Project 3/CADP/Frames'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hU5SXpVbNL05"
      },
      "source": [
        "# Actual Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcyFczgQQMNc"
      },
      "source": [
        "def load_set(img_path):\n",
        "  img = load_img(img_path)\n",
        "  tmp = skimage.color.rgb2gray(np.array(img))\n",
        "  tmp = transform.resize(tmp, (144, 256))\n",
        "  return tmp\n",
        "\n",
        "def horizontal_flip(img_path):\n",
        "  img = load_img(img_path)\n",
        "  tmp = skimage.color.rgb2gray(np.array(img))\n",
        "  tmp = skimage.transform.resize(tmp, (144, 256))\n",
        "  tmp = np.array(tmp)\n",
        "  tmp = np.flip(tmp, axis = 1)\n",
        "  return tmp\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US3aeBRWQMHl"
      },
      "source": [
        "def call_load_data(path):\n",
        "  count = 0\n",
        "  x = []\n",
        "\n",
        "  for files in os.listdir(path):\n",
        "    frames = []\n",
        "\n",
        "    img_path = str(path) + \"/\"+str(files.decode(\"utf-8\"))\n",
        "\n",
        "    if count < 99:\n",
        "      count = count + 1\n",
        "      img = load_set(img_path)\n",
        "    x.append(img)\n",
        "\n",
        "  return x\n",
        "\n",
        "def call_horizontal_flip(path):\n",
        "  count = 0\n",
        "  x = []\n",
        "  for files in os.listdir(path):\n",
        "    frames = []\n",
        "    img_path = str(path) + \"/\"+str(files.decode(\"utf-8\"))\n",
        "    if count < 99:\n",
        "      count = count + 1\n",
        "      img = horizontal_flip(img_path)\n",
        "    x.append(img)\n",
        "\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bP0CVi-BQMBX"
      },
      "source": [
        "def make_dataset(rand):\n",
        "  seq1 = np.zeros( (len(rand), 99, 144, 256)) \n",
        "  #seq1 = np.zeros( (2*len(rand), 99, 144, 256)) \n",
        " \n",
        "  #print(type(rand))\n",
        "  for i, fi in enumerate(rand):                   \n",
        "    print((i, fi))                           \n",
        "    if fi[-4:] == '.mp4' and i%2==0:\t\t\n",
        "      t = call_load_data(fi)             \n",
        "\n",
        "    elif fi[-4:] == '.mp4' and i%2==1:\n",
        "      t = call_horizontal_flip(fi)   \n",
        "       \n",
        "    if(len(t) == 99 and len(t[0]) == 144 and len(t[0][0]) == 256):                 \n",
        "      seq1[i] = t   \n",
        "                       \n",
        "    else:\n",
        "      print(\"Error\")\n",
        "      print('Dimensions are - ', len(t), len(t[0]), len(t[0][0]), sep = \" \")\n",
        "      pass  \n",
        "                      \n",
        "  print((seq1.shape))\n",
        "  return seq1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeUsV-UAdl0x"
      },
      "source": [
        "# Even More Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gcrGxYKkRUI"
      },
      "source": [
        "## Test Train Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4G1_Cu4JlOFn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3e7824ef-9dc9-4be7-e080-b018c2f14c88"
      },
      "source": [
        "##### split data into training and validation (sets and shuffle)\n",
        "x_train, x_test, y_train, y_test = train_test_split(all_files, labels, test_size=0.40, random_state=0)  \n",
        "x_train = np.asarray(x_train); y_train = np.asarray(y_train);                         "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "''"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AK8tgkSvKoY",
        "outputId": "61271ce8-662c-4434-adcd-f31d77609dd5"
      },
      "source": [
        "print(len(x_train))\n",
        "print(len(y_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "153\n",
            "153\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-B5GBH9caC4C",
        "outputId": "7ebed9ec-0705-4f85-8bf2-bfd458d0a447"
      },
      "source": [
        "print(len(x_test))\n",
        "print(len(y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "103\n",
            "103\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4ojtR6GXNjX"
      },
      "source": [
        "#Req libraries\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "from keras.layers import LSTM\n",
        "from keras.models import Model \n",
        "from keras.layers import Input,Dense,TimeDistributed\n",
        "\n",
        "from keras.layers import Dropout\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xCH_qlkLk6_"
      },
      "source": [
        "frame , row, col =(99,144,256)\n",
        "row_hidden = 128\n",
        "col_hidden = 128\n",
        "batch_size = 64       #256, 224, 128            #32 \n",
        "num_classes = 2\n",
        "epochs = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EUwFwAuNJlg"
      },
      "source": [
        "x =Input(shape=(frame, row, col))\n",
        "encoded_rows = TimeDistributed(LSTM(row_hidden))(x) \n",
        "encoded_columns =LSTM(col_hidden)(encoded_rows)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vy8wiKAP2Bv"
      },
      "source": [
        "layer2 = Dense(128, activation='relu')(encoded_columns)\n",
        "layer3 = Dropout(.2)(layer2)\n",
        "layer4 = Dense(64, activation='relu')(layer3)\n",
        "layer5 = Dropout(.2)(layer4)\n",
        "prediction = Dense(num_classes, activation='softmax')(layer5)\n",
        "\n",
        "model = Model(x, prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcYwS1x8kZ65",
        "outputId": "8a0e84d6-6486-4f1b-cc13-ebbbee2a0b6a"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='NAdam', metrics=['accuracy']) \n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 99, 144, 256)]    0         \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 99, 128)           197120    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 130       \n",
            "=================================================================\n",
            "Total params: 353,602\n",
            "Trainable params: 353,602\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxpbmgDG-qaW"
      },
      "source": [
        "## Creating testing and validation numpy arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pN5u8ypz_me",
        "outputId": "957561bd-9548-400d-94b6-427fe39e6483"
      },
      "source": [
        "x_testA = make_dataset(x_testA[0:batch_size])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/2.mp4')\n",
            "(1, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/56.mp4')\n",
            "(2, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/77.mp4')\n",
            "(3, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/60.mp4')\n",
            "(4, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/89.mp4')\n",
            "(5, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/110.mp4')\n",
            "(6, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/19.mp4')\n",
            "(7, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/108.mp4')\n",
            "(8, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/16.mp4')\n",
            "(9, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/48.mp4')\n",
            "(10, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/75.mp4')\n",
            "(11, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/8.mp4')\n",
            "(12, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/94.mp4')\n",
            "(13, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/63.mp4')\n",
            "(14, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/122.mp4')\n",
            "(15, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/34.mp4')\n",
            "(16, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/116.mp4')\n",
            "(17, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/54.mp4')\n",
            "(18, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/107.mp4')\n",
            "(19, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/140.mp4')\n",
            "(20, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/115.mp4')\n",
            "(21, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/109.mp4')\n",
            "(22, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/17.mp4')\n",
            "(23, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/95.mp4')\n",
            "(24, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/76.mp4')\n",
            "(25, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/90.mp4')\n",
            "(26, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/18.mp4')\n",
            "(27, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/1.mp4')\n",
            "(28, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/126.mp4')\n",
            "(29, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/62.mp4')\n",
            "(30, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/7.mp4')\n",
            "(31, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/53.mp4')\n",
            "(32, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/67.mp4')\n",
            "(33, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/27.mp4')\n",
            "(34, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/103.mp4')\n",
            "(35, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/52.mp4')\n",
            "(36, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/113.mp4')\n",
            "(37, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/41.mp4')\n",
            "(38, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/14.mp4')\n",
            "(39, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/5.mp4')\n",
            "(40, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/96.mp4')\n",
            "(41, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/25.mp4')\n",
            "(42, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/31.mp4')\n",
            "(43, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/111.mp4')\n",
            "(44, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/61.mp4')\n",
            "(45, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/57.mp4')\n",
            "(46, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/38.mp4')\n",
            "(47, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/6.mp4')\n",
            "(48, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/20.mp4')\n",
            "(49, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/69.mp4')\n",
            "(50, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/32.mp4')\n",
            "(51, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/55.mp4')\n",
            "(52, 99, 144, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAS9aM4jrhfv"
      },
      "source": [
        "#Saving Numpy Array x_testA\n",
        "os.chdir(\"/content/drive/MyDrive/Minor Project 3/\")\n",
        "\n",
        "np.save('x_testA.npy',x_testA)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_isxxYxINLP7",
        "outputId": "792a72d2-80f5-45cf-c949-f57ae3ac9f23"
      },
      "source": [
        "x_testB = make_dataset(x_testB[0:batch_size])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/55.mp4')\n",
            "(1, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/114.mp4')\n",
            "(2, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/12.mp4')\n",
            "(3, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/84.mp4')\n",
            "(4, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/50.mp4')\n",
            "(5, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/79.mp4')\n",
            "(6, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/117.mp4')\n",
            "(7, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/100.mp4')\n",
            "(8, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/64.mp4')\n",
            "(9, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/33.mp4')\n",
            "(10, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/46.mp4')\n",
            "(11, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/101.mp4')\n",
            "(12, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/91.mp4')\n",
            "(13, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/68.mp4')\n",
            "(14, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/47.mp4')\n",
            "(15, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/74.mp4')\n",
            "(16, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/99.mp4')\n",
            "(17, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/13.mp4')\n",
            "(18, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/53.mp4')\n",
            "(19, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/103.mp4')\n",
            "(20, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/67.mp4')\n",
            "(21, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/46.mp4')\n",
            "(22, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/131.mp4')\n",
            "(23, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/34.mp4')\n",
            "(24, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/89.mp4')\n",
            "(25, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/130.mp4')\n",
            "(26, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/9.mp4')\n",
            "(27, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/56.mp4')\n",
            "(28, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/83.mp4')\n",
            "(29, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/38.mp4')\n",
            "(30, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/75.mp4')\n",
            "(31, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/25.mp4')\n",
            "(32, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/110.mp4')\n",
            "(33, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/77.mp4')\n",
            "(34, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/11.mp4')\n",
            "(35, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/42.mp4')\n",
            "(36, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/66.mp4')\n",
            "(37, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/6.mp4')\n",
            "(38, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/23.mp4')\n",
            "(39, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/70.mp4')\n",
            "(40, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/9.mp4')\n",
            "(41, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/13.mp4')\n",
            "(42, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/65.mp4')\n",
            "(43, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/21.mp4')\n",
            "(44, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/93.mp4')\n",
            "(45, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/27.mp4')\n",
            "(46, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/116.mp4')\n",
            "(47, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/45.mp4')\n",
            "(48, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/65.mp4')\n",
            "(49, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/61.mp4')\n",
            "(50, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/72.mp4')\n",
            "(51, 99, 144, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBkln2q3vfDl"
      },
      "source": [
        "#Saving Numpy Array x_testB\n",
        "os.chdir(\"/content/drive/MyDrive/Minor Project 3/\")\n",
        "np.save('x_testB.npy', x_testB)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2F6bwQsaQ49"
      },
      "source": [
        "## Creating and saving x_test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7na1VHLTaQH6"
      },
      "source": [
        "x_test_batch_list = [x_test[i:i + batch_size] for i in range(0, len(x_test), batch_size)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x40QVMopdeUE",
        "outputId": "7d5b7a28-d65f-4194-9ce6-bdef2f4a5ea2"
      },
      "source": [
        "len(x_test_batch_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwXM-DHscByv",
        "outputId": "6ab0677a-85b5-40be-c3cb-59a7aded958e"
      },
      "source": [
        "x_test_batch = []\n",
        "os.chdir(\"/content/drive/MyDrive/Minor Project 3/Model 64/\")\n",
        "for i in range(len(x_test_batch_list)):\n",
        "  x_test_batch = make_dataset(x_test[0:batch_size])\n",
        "  np.save('x_test_batch['+str(i)+'].npy', x_test_batch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/3.mp4')\n",
            "(1, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/114.mp4')\n",
            "(2, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/18.mp4')\n",
            "(3, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/84.mp4')\n",
            "(4, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/54.mp4')\n",
            "(5, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/93.mp4')\n",
            "(6, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/7.mp4')\n",
            "(7, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/133.mp4')\n",
            "(8, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/64.mp4')\n",
            "(9, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/36.mp4')\n",
            "(10, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/51.mp4')\n",
            "(11, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/132.mp4')\n",
            "(12, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/91.mp4')\n",
            "(13, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/74.mp4')\n",
            "(14, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/56.mp4')\n",
            "(15, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/74.mp4')\n",
            "(16, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/97.mp4')\n",
            "(17, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/19.mp4')\n",
            "(18, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/1.mp4')\n",
            "(19, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/103.mp4')\n",
            "(20, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/73.mp4')\n",
            "(21, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/46.mp4')\n",
            "(22, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/126.mp4')\n",
            "(23, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/43.mp4')\n",
            "(24, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/89.mp4')\n",
            "(25, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/118.mp4')\n",
            "(26, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/9.mp4')\n",
            "(27, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/56.mp4')\n",
            "(28, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/111.mp4')\n",
            "(29, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/38.mp4')\n",
            "(30, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/81.mp4')\n",
            "(31, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/30.mp4')\n",
            "(32, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/110.mp4')\n",
            "(33, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/90.mp4')\n",
            "(34, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/16.mp4')\n",
            "(35, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/49.mp4')\n",
            "(36, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/72.mp4')\n",
            "(37, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/6.mp4')\n",
            "(38, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/23.mp4')\n",
            "(39, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/76.mp4')\n",
            "(40, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/31.mp4')\n",
            "(41, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/13.mp4')\n",
            "(42, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/71.mp4')\n",
            "(43, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/64.mp4')\n",
            "(44, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/93.mp4')\n",
            "(45, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/33.mp4')\n",
            "(46, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/107.mp4')\n",
            "(47, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/45.mp4')\n",
            "(48, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/65.mp4')\n",
            "(49, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/67.mp4')\n",
            "(50, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/72.mp4')\n",
            "(51, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/11.mp4')\n",
            "(52, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/22.mp4')\n",
            "(53, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/77.mp4')\n",
            "(54, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/60.mp4')\n",
            "(55, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/88.mp4')\n",
            "(56, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/138.mp4')\n",
            "(57, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/19.mp4')\n",
            "(58, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/105.mp4')\n",
            "(59, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/16.mp4')\n",
            "(60, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/55.mp4')\n",
            "(61, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/75.mp4')\n",
            "(62, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/8.mp4')\n",
            "(63, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/94.mp4')\n",
            "(64, 99, 144, 256)\n",
            "(0, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/3.mp4')\n",
            "(1, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/114.mp4')\n",
            "(2, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/18.mp4')\n",
            "(3, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/84.mp4')\n",
            "(4, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/54.mp4')\n",
            "(5, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/93.mp4')\n",
            "(6, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/7.mp4')\n",
            "(7, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/133.mp4')\n",
            "(8, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/64.mp4')\n",
            "(9, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/36.mp4')\n",
            "(10, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/51.mp4')\n",
            "(11, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/132.mp4')\n",
            "(12, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/91.mp4')\n",
            "(13, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/74.mp4')\n",
            "(14, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/56.mp4')\n",
            "(15, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/74.mp4')\n",
            "(16, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/97.mp4')\n",
            "(17, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/19.mp4')\n",
            "(18, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/1.mp4')\n",
            "(19, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/103.mp4')\n",
            "(20, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/73.mp4')\n",
            "(21, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/46.mp4')\n",
            "(22, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/126.mp4')\n",
            "(23, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/43.mp4')\n",
            "(24, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/89.mp4')\n",
            "(25, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/118.mp4')\n",
            "(26, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/9.mp4')\n",
            "(27, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/56.mp4')\n",
            "(28, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/111.mp4')\n",
            "(29, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/38.mp4')\n",
            "(30, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/81.mp4')\n",
            "(31, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/30.mp4')\n",
            "(32, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/110.mp4')\n",
            "(33, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/90.mp4')\n",
            "(34, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/16.mp4')\n",
            "(35, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/49.mp4')\n",
            "(36, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/72.mp4')\n",
            "(37, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/6.mp4')\n",
            "(38, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/23.mp4')\n",
            "(39, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/76.mp4')\n",
            "(40, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/31.mp4')\n",
            "(41, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/13.mp4')\n",
            "(42, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/71.mp4')\n",
            "(43, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/64.mp4')\n",
            "(44, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/93.mp4')\n",
            "(45, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/33.mp4')\n",
            "(46, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/107.mp4')\n",
            "(47, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/45.mp4')\n",
            "(48, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/65.mp4')\n",
            "(49, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/67.mp4')\n",
            "(50, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/72.mp4')\n",
            "(51, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/11.mp4')\n",
            "(52, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/22.mp4')\n",
            "(53, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/77.mp4')\n",
            "(54, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/60.mp4')\n",
            "(55, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/88.mp4')\n",
            "(56, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/138.mp4')\n",
            "(57, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/19.mp4')\n",
            "(58, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/105.mp4')\n",
            "(59, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/16.mp4')\n",
            "(60, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/55.mp4')\n",
            "(61, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/75.mp4')\n",
            "(62, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/8.mp4')\n",
            "(63, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/94.mp4')\n",
            "(64, 99, 144, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soRflAJIaWmN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L73Vx1kT-z0P"
      },
      "source": [
        "## Loading x_testA and x_testB from .npy files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6Jk1Yz9-ziy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f07af7e-f0c1-4183-8dd4-05549ab5a71a"
      },
      "source": [
        "os.chdir('/content/drive/MyDrive/Minor Project 3/')\n",
        "x_testA = np.load('x_testA.npy')\n",
        "x_testA.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(52, 99, 144, 256)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvefH818_yp8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97ea5b4f-59da-4625-8759-cc935ae6c577"
      },
      "source": [
        "x_testB = np.load('x_testB.npy')\n",
        "x_testB.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(51, 99, 144, 256)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVu-6i-Ya11R"
      },
      "source": [
        "#x_test\n",
        "x_test = np.load('x_test.npy')\n",
        "x_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_oFFTA6AECx"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7er9Livcknjg"
      },
      "source": [
        "os.chdir(\"/content/drive/MyDrive/Minor Project 3/\")\n",
        "i=0; filepath='HRNN_pretrained_model.hdf5'\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "np.random.seed(1642)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tg-bBPNBV73n"
      },
      "source": [
        "for i in range(0, epochs): \n",
        "  c = list(zip(x_train, y_train)) \n",
        "  random.shuffle(c) \n",
        "\n",
        "x_shuff, y_shuff = zip(*c) \n",
        "x_shuff = np.array(x_shuff)\n",
        "y_shuff=np.array(y_shuff) \n",
        "\n",
        "x_batch = [x_shuff[i:i + batch_size] for i in range(0, len(x_shuff), batch_size)] \n",
        "y_batch = [y_shuff[i:i + batch_size] for i in range(0, len(x_shuff), batch_size)] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaSqdLKpwCNz"
      },
      "source": [
        "print('x_batch length = ',len(x_batch))\n",
        "print('y_batch length = ',len(y_batch))\n",
        "\n",
        "print('x_batch[0] length = ',len(x_batch[0]))\n",
        "print('y_batch[0] length = ',len(x_batch[0]))\n",
        "\n",
        "print('x_testB type = ',type(x_testB))\n",
        "print('y_testB type = ',type(y_testB))\n",
        "\n",
        "print('y_testB length = ', len(y_testB))\n",
        "print('x_testB shape = ', x_testB.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhtoJmEMWVCs"
      },
      "source": [
        "Prepocessing different batches of x_train and y_train and then storing them in Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeOBqfDOV7yz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89d9408e-ada7-42d6-8aa1-619bc46f9c6c"
      },
      "source": [
        "os.chdir(\"/content/drive/MyDrive/Minor Project 3/Model 64/\")\n",
        "for i in range(len(x_batch)):\n",
        "  xx = make_dataset(x_batch[i])\n",
        "  np.save('xx['+str(i)+'].npy', xx)\n",
        "  yy = y_batch[i]\n",
        "  np.save('yy['+str(i)+'].npy', yy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/50.mp4')\n",
            "(1, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/12.mp4')\n",
            "(2, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/88.mp4')\n",
            "(3, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/30.mp4')\n",
            "(4, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/7.mp4')\n",
            "(5, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/85.mp4')\n",
            "(6, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/108.mp4')\n",
            "(7, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/140.mp4')\n",
            "(8, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/78.mp4')\n",
            "(9, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/128.mp4')\n",
            "(10, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/48.mp4')\n",
            "(11, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/100.mp4')\n",
            "(12, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/47.mp4')\n",
            "(13, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/11.mp4')\n",
            "(14, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/79.mp4')\n",
            "(15, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/77.mp4')\n",
            "(16, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/98.mp4')\n",
            "(17, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/106.mp4')\n",
            "(18, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/87.mp4')\n",
            "(19, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/99.mp4')\n",
            "(20, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/25.mp4')\n",
            "(21, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/28.mp4')\n",
            "(22, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/113.mp4')\n",
            "(23, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/26.mp4')\n",
            "(24, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/21.mp4')\n",
            "(25, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/115.mp4')\n",
            "(26, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/70.mp4')\n",
            "(27, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/44.mp4')\n",
            "(28, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/100.mp4')\n",
            "(29, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/102.mp4')\n",
            "(30, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/29.mp4')\n",
            "(31, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/114.mp4')\n",
            "(32, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/37.mp4')\n",
            "(33, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/29.mp4')\n",
            "(34, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/70.mp4')\n",
            "(35, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/104.mp4')\n",
            "(36, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/110.mp4')\n",
            "(37, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/43.mp4')\n",
            "(38, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/40.mp4')\n",
            "(39, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/107.mp4')\n",
            "(40, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/33.mp4')\n",
            "(41, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/135.mp4')\n",
            "(42, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/36.mp4')\n",
            "(43, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/71.mp4')\n",
            "(44, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/49.mp4')\n",
            "(45, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/35.mp4')\n",
            "(46, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/10.mp4')\n",
            "(47, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/18.mp4')\n",
            "(48, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/101.mp4')\n",
            "(49, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/52.mp4')\n",
            "(50, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/89.mp4')\n",
            "(51, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/81.mp4')\n",
            "(52, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/59.mp4')\n",
            "(53, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/131.mp4')\n",
            "(54, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/59.mp4')\n",
            "(55, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/24.mp4')\n",
            "(56, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/97.mp4')\n",
            "(57, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/95.mp4')\n",
            "(58, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/66.mp4')\n",
            "(59, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/98.mp4')\n",
            "(60, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/52.mp4')\n",
            "(61, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/20.mp4')\n",
            "(62, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/22.mp4')\n",
            "(63, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/2.mp4')\n",
            "(64, 99, 144, 256)\n",
            "(0, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/45.mp4')\n",
            "(1, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/63.mp4')\n",
            "(2, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/14.mp4')\n",
            "(3, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/105.mp4')\n",
            "(4, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/124.mp4')\n",
            "(5, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/68.mp4')\n",
            "(6, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/108.mp4')\n",
            "(7, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/94.mp4')\n",
            "(8, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/83.mp4')\n",
            "(9, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/96.mp4')\n",
            "(10, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/17.mp4')\n",
            "(11, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/123.mp4')\n",
            "(12, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/134.mp4')\n",
            "(13, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/21.mp4')\n",
            "(14, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/37.mp4')\n",
            "(15, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/109.mp4')\n",
            "(16, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/38.mp4')\n",
            "(17, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/104.mp4')\n",
            "(18, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/115.mp4')\n",
            "(19, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/35.mp4')\n",
            "(20, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/51.mp4')\n",
            "(21, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/5.mp4')\n",
            "(22, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/10.mp4')\n",
            "(23, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/4.mp4')\n",
            "(24, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/120.mp4')\n",
            "(25, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/73.mp4')\n",
            "(26, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/80.mp4')\n",
            "(27, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/80.mp4')\n",
            "(28, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/4.mp4')\n",
            "(29, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/66.mp4')\n",
            "(30, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/13.mp4')\n",
            "(31, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/57.mp4')\n",
            "(32, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/46.mp4')\n",
            "(33, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/32.mp4')\n",
            "(34, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/65.mp4')\n",
            "(35, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/24.mp4')\n",
            "(36, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/3.mp4')\n",
            "(37, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/41.mp4')\n",
            "(38, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/82.mp4')\n",
            "(39, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/8.mp4')\n",
            "(40, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/106.mp4')\n",
            "(41, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/39.mp4')\n",
            "(42, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/130.mp4')\n",
            "(43, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/85.mp4')\n",
            "(44, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/68.mp4')\n",
            "(45, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/28.mp4')\n",
            "(46, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/54.mp4')\n",
            "(47, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/91.mp4')\n",
            "(48, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/119.mp4')\n",
            "(49, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/48.mp4')\n",
            "(50, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/62.mp4')\n",
            "(51, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/83.mp4')\n",
            "(52, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/122.mp4')\n",
            "(53, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/27.mp4')\n",
            "(54, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/121.mp4')\n",
            "(55, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/50.mp4')\n",
            "(56, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/44.mp4')\n",
            "(57, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/47.mp4')\n",
            "(58, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/32.mp4')\n",
            "(59, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/125.mp4')\n",
            "(60, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/116.mp4')\n",
            "(61, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/99.mp4')\n",
            "(62, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/15.mp4')\n",
            "(63, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/84.mp4')\n",
            "(64, 99, 144, 256)\n",
            "(0, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/60.mp4')\n",
            "(1, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/82.mp4')\n",
            "(2, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/136.mp4')\n",
            "(3, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/92.mp4')\n",
            "(4, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/42.mp4')\n",
            "(5, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/53.mp4')\n",
            "(6, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/34.mp4')\n",
            "(7, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/63.mp4')\n",
            "(8, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/9.mp4')\n",
            "(9, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/78.mp4')\n",
            "(10, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/102.mp4')\n",
            "(11, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/111.mp4')\n",
            "(12, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/87.mp4')\n",
            "(13, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/15.mp4')\n",
            "(14, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/12.mp4')\n",
            "(15, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/112.mp4')\n",
            "(16, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/117.mp4')\n",
            "(17, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/39.mp4')\n",
            "(18, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/58.mp4')\n",
            "(19, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/92.mp4')\n",
            "(20, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/79.mp4')\n",
            "(21, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/113.mp4')\n",
            "(22, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/86.mp4')\n",
            "(23, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Negative/137.mp4')\n",
            "(24, '/content/drive/MyDrive/Minor Project 3/CADP/Frames/Positive/69.mp4')\n",
            "(25, 99, 144, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUMBGCe4WfvP"
      },
      "source": [
        "Prepocessing different batches of y_test and storing them in Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3NxTxCee--z",
        "outputId": "6ce46150-5a3a-43a4-fcce-88c6aecabdbd"
      },
      "source": [
        "os.chdir(\"/content/drive/MyDrive/Minor Project 3/Model 64/\")\n",
        "y_test_temp = [y_test[i:i + batch_size] for i in range(0, len(y_test), batch_size)] \n",
        "y_testB_temp = np.array(y_test_temp)\n",
        "np.save('y_test_temp.npy',y_test_temp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order, subok=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dm1JpVbTfJsA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFZrZzGesNxg"
      },
      "source": [
        "## Saving xx and yy for Model 256"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9H3r9fvcz92"
      },
      "source": [
        "os.chdir(\"/content/drive/MyDrive/Minor Project 3/\")\n",
        "np.save('xx.npy',xx)\n",
        "np.save('yy.npy', yy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gd8pk-UvsQ8z"
      },
      "source": [
        "## Loading xx and yy for Model 256"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NghursqXsHe5"
      },
      "source": [
        "os.chdir(\"/content/drive/MyDrive/Minor Project 3/\")\n",
        "xx = np.load('xx.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuRn8gAHWWpA"
      },
      "source": [
        "yy = np.load('yy.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zG4L6jJEXJJE",
        "outputId": "8661f70f-5d84-4e99-f24e-2b2d8d095815"
      },
      "source": [
        "xx.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(153, 99, 144, 256)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pjUJfODXNTb",
        "outputId": "36339127-7fb9-4d9e-a3e9-4bfe091014b1"
      },
      "source": [
        "yy.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(153, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuTCXiX9eNWi"
      },
      "source": [
        "y_test_temp1 = np.array(y_test[0:batch_size])\n",
        "y_test_temp2 = np.array(y_test[38:102])\n",
        "y_test_np = np.concatenate((y_test_temp1, y_test_temp2), axis = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvG6lz-2eesS"
      },
      "source": [
        "np.save('y_test_np.npy',y_test_np)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvIKg8YLbOb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3aac2c13-692c-4c65-ecc6-03e1a391fce3"
      },
      "source": [
        "y_test = np.load('y_test_np.npy')\n",
        "y_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(128, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK9rsoSNAPqG"
      },
      "source": [
        "# Model 256"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htDfPMjufugt"
      },
      "source": [
        "os.chdir(\"/content/drive/MyDrive/Minor Project 3/\")\n",
        "\n",
        "xx = np.load('xx.npy')\n",
        "yy = np.load('yy.npy') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwhOuR35fuYi"
      },
      "source": [
        "model.fit(xx, yy, batch_size=len(xx),epochs=10, callbacks=callbacks_list)\n",
        "\n",
        "del xx\n",
        "del yy\n",
        "\n",
        "modelfilename = '/content/drive/MyDrive/Minor Project 3/Checkpoints/model_256.h5' \n",
        "model.save_weights(modelfilename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gNbF-tLXFNH"
      },
      "source": [
        "## Making and storing model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sXGXDDUSEDl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f13cb63-22c5-4f66-98bd-744549701125"
      },
      "source": [
        "model.fit(xx, yy, batch_size=len(xx),epochs=10, validation_data=(x_testB, y_testB_temp), callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tv_13Q1Skg_C"
      },
      "source": [
        "#Save Model Weights\n",
        "path = '/content/drive/MyDrive/Minor Project 3/Checkpoints/model_v2.h5'\n",
        "\n",
        "model.save_weights(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXou-V-wfsbW"
      },
      "source": [
        "# Model 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZPt3so_XRy_"
      },
      "source": [
        "## Success 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueNaY3Y8H4i8",
        "outputId": "6886dc97-fc43-468f-fad4-8a0cebb0c20e"
      },
      "source": [
        "os.chdir(\"/content/drive/MyDrive/Minor Project 3/Model 64/\")\n",
        "for i in range(3):\n",
        "  xx = np.load('xx['+str(i)+'].npy')\n",
        "  yy = np.load('yy['+str(i)+'].npy')\n",
        "\n",
        "  model.fit(xx, yy, batch_size=len(xx),epochs=30, callbacks=callbacks_list)\n",
        "  #model.fit(xx, yy, batch_size=len(xx),epochs=30)\n",
        "\n",
        "  del xx\n",
        "  del yy\n",
        "\n",
        "  modelfilename = '/content/drive/MyDrive/Minor Project 3/Checkpoints/model_64['+str(i)+'].h5' \n",
        "  model.save_weights(modelfilename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "1/1 [==============================] - 1s 854ms/step - loss: 0.6766 - accuracy: 0.6562\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 2/30\n",
            "1/1 [==============================] - 1s 502ms/step - loss: 0.5804 - accuracy: 0.7188\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 3/30\n",
            "1/1 [==============================] - 1s 539ms/step - loss: 0.5659 - accuracy: 0.7344\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 4/30\n",
            "1/1 [==============================] - 0s 438ms/step - loss: 0.7089 - accuracy: 0.4844\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 5/30\n",
            "1/1 [==============================] - 1s 592ms/step - loss: 0.6831 - accuracy: 0.5156\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 6/30\n",
            "1/1 [==============================] - 0s 487ms/step - loss: 0.6270 - accuracy: 0.6562\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 7/30\n",
            "1/1 [==============================] - 0s 485ms/step - loss: 0.6211 - accuracy: 0.6094\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 8/30\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 0.6757 - accuracy: 0.4844\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 9/30\n",
            "1/1 [==============================] - 1s 536ms/step - loss: 0.6373 - accuracy: 0.6250\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 10/30\n",
            "1/1 [==============================] - 0s 500ms/step - loss: 0.6190 - accuracy: 0.6094\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 11/30\n",
            "1/1 [==============================] - 0s 500ms/step - loss: 0.6020 - accuracy: 0.6406\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 12/30\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 0.6135 - accuracy: 0.6250\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 13/30\n",
            "1/1 [==============================] - 1s 506ms/step - loss: 0.5749 - accuracy: 0.7031\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 14/30\n",
            "1/1 [==============================] - 0s 496ms/step - loss: 0.5642 - accuracy: 0.7031\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 15/30\n",
            "1/1 [==============================] - 1s 519ms/step - loss: 0.5542 - accuracy: 0.7031\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 16/30\n",
            "1/1 [==============================] - 1s 509ms/step - loss: 0.6406 - accuracy: 0.6250\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 17/30\n",
            "1/1 [==============================] - 1s 536ms/step - loss: 0.7197 - accuracy: 0.5000\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 18/30\n",
            "1/1 [==============================] - 0s 469ms/step - loss: 0.6241 - accuracy: 0.5156\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 19/30\n",
            "1/1 [==============================] - 1s 519ms/step - loss: 0.6249 - accuracy: 0.6250\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 20/30\n",
            "1/1 [==============================] - 0s 494ms/step - loss: 0.6006 - accuracy: 0.7344\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 21/30\n",
            "1/1 [==============================] - 0s 485ms/step - loss: 0.5767 - accuracy: 0.7500\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 22/30\n",
            "1/1 [==============================] - 0s 498ms/step - loss: 0.5630 - accuracy: 0.6562\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 23/30\n",
            "1/1 [==============================] - 1s 505ms/step - loss: 0.5302 - accuracy: 0.7344\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 24/30\n",
            "1/1 [==============================] - 0s 491ms/step - loss: 0.5095 - accuracy: 0.7344\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 25/30\n",
            "1/1 [==============================] - 0s 486ms/step - loss: 0.5026 - accuracy: 0.7656\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 26/30\n",
            "1/1 [==============================] - 0s 493ms/step - loss: 0.8786 - accuracy: 0.6094\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 27/30\n",
            "1/1 [==============================] - 1s 526ms/step - loss: 0.7262 - accuracy: 0.4844\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 28/30\n",
            "1/1 [==============================] - 0s 472ms/step - loss: 0.6329 - accuracy: 0.6406\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 29/30\n",
            "1/1 [==============================] - 0s 496ms/step - loss: 0.6197 - accuracy: 0.6250\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 30/30\n",
            "1/1 [==============================] - 0s 456ms/step - loss: 0.6248 - accuracy: 0.6562\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 1/30\n",
            "1/1 [==============================] - 1s 794ms/step - loss: 0.7422 - accuracy: 0.4062\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 2/30\n",
            "1/1 [==============================] - 0s 445ms/step - loss: 0.5616 - accuracy: 0.7656\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 3/30\n",
            "1/1 [==============================] - 1s 529ms/step - loss: 0.5481 - accuracy: 0.7656\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 4/30\n",
            "1/1 [==============================] - 0s 440ms/step - loss: 0.4754 - accuracy: 0.7812\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 5/30\n",
            "1/1 [==============================] - 1s 514ms/step - loss: 0.4703 - accuracy: 0.7969\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 6/30\n",
            "1/1 [==============================] - 0s 498ms/step - loss: 0.4516 - accuracy: 0.8125\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 7/30\n",
            "1/1 [==============================] - 1s 523ms/step - loss: 0.4338 - accuracy: 0.8125\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 8/30\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 0.4300 - accuracy: 0.8125\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 9/30\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 0.4260 - accuracy: 0.8125\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 10/30\n",
            "1/1 [==============================] - 0s 476ms/step - loss: 0.3892 - accuracy: 0.8125\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 11/30\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 0.3857 - accuracy: 0.8125\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 12/30\n",
            "1/1 [==============================] - 0s 498ms/step - loss: 0.3793 - accuracy: 0.8438\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 13/30\n",
            "1/1 [==============================] - 1s 516ms/step - loss: 0.4013 - accuracy: 0.8281\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 14/30\n",
            "1/1 [==============================] - 0s 489ms/step - loss: 0.3614 - accuracy: 0.8438\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 15/30\n",
            "1/1 [==============================] - 1s 527ms/step - loss: 0.4598 - accuracy: 0.8125\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 16/30\n",
            "1/1 [==============================] - 1s 500ms/step - loss: 0.4372 - accuracy: 0.8438\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 17/30\n",
            "1/1 [==============================] - 1s 513ms/step - loss: 0.4469 - accuracy: 0.8125\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 18/30\n",
            "1/1 [==============================] - 0s 478ms/step - loss: 0.3979 - accuracy: 0.8438\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 19/30\n",
            "1/1 [==============================] - 1s 515ms/step - loss: 0.4202 - accuracy: 0.8281\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 20/30\n",
            "1/1 [==============================] - 1s 501ms/step - loss: 0.3583 - accuracy: 0.8750\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 21/30\n",
            "1/1 [==============================] - 1s 512ms/step - loss: 0.3501 - accuracy: 0.8281\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 22/30\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 0.3694 - accuracy: 0.8594\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 23/30\n",
            "1/1 [==============================] - 0s 492ms/step - loss: 0.3462 - accuracy: 0.8594\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 24/30\n",
            "1/1 [==============================] - 1s 509ms/step - loss: 0.3322 - accuracy: 0.8594\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 25/30\n",
            "1/1 [==============================] - 0s 489ms/step - loss: 0.3814 - accuracy: 0.8594\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 26/30\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 0.4339 - accuracy: 0.8125\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 27/30\n",
            "1/1 [==============================] - 1s 503ms/step - loss: 0.4942 - accuracy: 0.8125\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 28/30\n",
            "1/1 [==============================] - 1s 512ms/step - loss: 0.3652 - accuracy: 0.8750\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 29/30\n",
            "1/1 [==============================] - 1s 516ms/step - loss: 0.3456 - accuracy: 0.8594\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 30/30\n",
            "1/1 [==============================] - 0s 444ms/step - loss: 0.3404 - accuracy: 0.8750\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 1/30\n",
            "1/1 [==============================] - 0s 342ms/step - loss: 1.3489 - accuracy: 0.4000\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 2/30\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.7130 - accuracy: 0.4800\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 3/30\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.6515 - accuracy: 0.6400\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 4/30\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.5964 - accuracy: 0.6800\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 5/30\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.5887 - accuracy: 0.6800\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 6/30\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 0.5778 - accuracy: 0.6800\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 7/30\n",
            "1/1 [==============================] - 0s 204ms/step - loss: 0.5428 - accuracy: 0.6800\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 8/30\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 0.5063 - accuracy: 0.6800\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 9/30\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 0.5083 - accuracy: 0.7600\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 10/30\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 0.9789 - accuracy: 0.6400\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 11/30\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.5489 - accuracy: 0.7200\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 12/30\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 0.5557 - accuracy: 0.6800\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 13/30\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.5717 - accuracy: 0.8000\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 14/30\n",
            "1/1 [==============================] - 0s 199ms/step - loss: 0.5529 - accuracy: 0.6800\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 15/30\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 0.5286 - accuracy: 0.8000\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 16/30\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.4835 - accuracy: 0.7200\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 17/30\n",
            "1/1 [==============================] - 0s 204ms/step - loss: 0.5130 - accuracy: 0.8400\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 18/30\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 0.5724 - accuracy: 0.6800\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 19/30\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.5560 - accuracy: 0.6800\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 20/30\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.4939 - accuracy: 0.8000\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 21/30\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.4545 - accuracy: 0.7600\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 22/30\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.4165 - accuracy: 0.8000\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 23/30\n",
            "1/1 [==============================] - 0s 204ms/step - loss: 0.4749 - accuracy: 0.8400\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 24/30\n",
            "1/1 [==============================] - 0s 197ms/step - loss: 0.6097 - accuracy: 0.7200\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 25/30\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 0.5421 - accuracy: 0.6000\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 26/30\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.4634 - accuracy: 0.8400\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 27/30\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 0.4391 - accuracy: 0.7200\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 28/30\n",
            "1/1 [==============================] - 0s 199ms/step - loss: 0.3860 - accuracy: 0.8800\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 29/30\n",
            "1/1 [==============================] - 0s 204ms/step - loss: 0.4105 - accuracy: 0.8000\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 30/30\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.5114 - accuracy: 0.7200\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEF4XNWLXXzz"
      },
      "source": [
        "## Success 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IZZxeoLGgGk",
        "outputId": "2ef4d322-015d-4433-fa52-8de1c2561fff"
      },
      "source": [
        "os.chdir(\"/content/drive/MyDrive/Minor Project 3/Model 64/\")\n",
        "for i in range(3):\n",
        "  xx = np.load('xx['+str(i)+'].npy')\n",
        "  yy = np.load('yy['+str(i)+'].npy')\n",
        "\n",
        "  model.fit(xx, yy, batch_size=len(xx),epochs=20, callbacks=callbacks_list)\n",
        "  #model.fit(xx, yy, batch_size=len(xx),epochs=30)\n",
        "\n",
        "  del xx\n",
        "  del yy\n",
        "\n",
        "  modelfilename = '/content/drive/MyDrive/Minor Project 3/Checkpoints/model_64['+str(i)+'].h5' \n",
        "  model.save_weights(modelfilename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1/1 [==============================] - 1s 884ms/step - loss: 0.7822 - accuracy: 0.5625\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 428ms/step - loss: 0.7313 - accuracy: 0.4375\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 434ms/step - loss: 0.6786 - accuracy: 0.6250\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 419ms/step - loss: 0.6650 - accuracy: 0.6406\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 487ms/step - loss: 0.6696 - accuracy: 0.6094\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 1s 505ms/step - loss: 0.6621 - accuracy: 0.6562\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 0.6574 - accuracy: 0.6094\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 464ms/step - loss: 0.6293 - accuracy: 0.5781\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 498ms/step - loss: 0.6271 - accuracy: 0.6875\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 1s 511ms/step - loss: 0.6753 - accuracy: 0.6094\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 474ms/step - loss: 0.6614 - accuracy: 0.5312\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 498ms/step - loss: 0.6197 - accuracy: 0.6250\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 1s 517ms/step - loss: 0.5895 - accuracy: 0.7031\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 0.6019 - accuracy: 0.6094\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 485ms/step - loss: 0.7212 - accuracy: 0.4688\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 0s 492ms/step - loss: 0.6197 - accuracy: 0.6094\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 0s 486ms/step - loss: 0.6478 - accuracy: 0.6250\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 18/20\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 0.6369 - accuracy: 0.6250\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 19/20\n",
            "1/1 [==============================] - 1s 510ms/step - loss: 0.5756 - accuracy: 0.7031\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 20/20\n",
            "1/1 [==============================] - 0s 440ms/step - loss: 0.5924 - accuracy: 0.6250\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 1s 792ms/step - loss: 0.6069 - accuracy: 0.7969\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 460ms/step - loss: 0.5568 - accuracy: 0.7656\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 430ms/step - loss: 0.5265 - accuracy: 0.7656\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 436ms/step - loss: 0.5233 - accuracy: 0.7812\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 492ms/step - loss: 0.5251 - accuracy: 0.7656\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 490ms/step - loss: 0.5148 - accuracy: 0.7812\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 487ms/step - loss: 0.5549 - accuracy: 0.7656\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 1s 519ms/step - loss: 0.5161 - accuracy: 0.7812\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 1s 511ms/step - loss: 0.4682 - accuracy: 0.7656\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 0.4715 - accuracy: 0.7812\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 1s 520ms/step - loss: 0.4622 - accuracy: 0.7812\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 468ms/step - loss: 0.4583 - accuracy: 0.8125\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 489ms/step - loss: 0.4613 - accuracy: 0.7969\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 1s 520ms/step - loss: 0.5053 - accuracy: 0.7812\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 1s 524ms/step - loss: 0.5896 - accuracy: 0.7969\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 0.4342 - accuracy: 0.7969\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 0.4191 - accuracy: 0.7969\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 18/20\n",
            "1/1 [==============================] - 0s 487ms/step - loss: 0.4082 - accuracy: 0.8125\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 19/20\n",
            "1/1 [==============================] - 0s 463ms/step - loss: 0.3857 - accuracy: 0.7969\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 20/20\n",
            "1/1 [==============================] - 0s 429ms/step - loss: 0.3705 - accuracy: 0.8438\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 338ms/step - loss: 1.7395 - accuracy: 0.4000\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.6858 - accuracy: 0.5200\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.5532 - accuracy: 0.6800\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.5433 - accuracy: 0.6800\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.5202 - accuracy: 0.6800\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 0.5312 - accuracy: 0.6800\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 0.4938 - accuracy: 0.6800\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 0.5688 - accuracy: 0.6400\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 199ms/step - loss: 0.4936 - accuracy: 0.6800\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 0.5024 - accuracy: 0.6800\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 200ms/step - loss: 0.4793 - accuracy: 0.8000\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.4687 - accuracy: 0.7200\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 204ms/step - loss: 0.5025 - accuracy: 0.7200\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.4454 - accuracy: 0.8000\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 2.1659 - accuracy: 0.6800\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 0s 199ms/step - loss: 0.5972 - accuracy: 0.6400\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 0.5757 - accuracy: 0.6800\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 18/20\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.5783 - accuracy: 0.6000\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 19/20\n",
            "1/1 [==============================] - 0s 198ms/step - loss: 0.5125 - accuracy: 0.7600\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 20/20\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.4765 - accuracy: 0.7200\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUz3iTW6Xalp"
      },
      "source": [
        "## Success 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0_aBRf3fum1",
        "outputId": "b06183ee-090c-4f8a-e45d-673444d0ab04"
      },
      "source": [
        "os.chdir(\"/content/drive/MyDrive/Minor Project 3/Model 64/\")\n",
        "for i in range(3):\n",
        "  xx = np.load('xx['+str(i)+'].npy')\n",
        "  yy = np.load('yy['+str(i)+'].npy')\n",
        "\n",
        "  #model.fit(xx, yy, batch_size=len(xx),epochs=30, callbacks=callbacks_list)\n",
        "  model.fit(xx, yy, batch_size=len(xx),epochs=30)\n",
        "\n",
        "  del xx\n",
        "  del yy\n",
        "\n",
        "  modelfilename = '/content/drive/MyDrive/Minor Project 3/Checkpoints/model_64['+str(i)+'].h5' \n",
        "  model.save_weights(modelfilename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "1/1 [==============================] - 19s 19s/step - loss: 0.7141 - accuracy: 0.4375\n",
            "Epoch 2/30\n",
            "1/1 [==============================] - 1s 550ms/step - loss: 0.6931 - accuracy: 0.5625\n",
            "Epoch 3/30\n",
            "1/1 [==============================] - 0s 499ms/step - loss: 0.7005 - accuracy: 0.5156\n",
            "Epoch 4/30\n",
            "1/1 [==============================] - 1s 533ms/step - loss: 0.6697 - accuracy: 0.5469\n",
            "Epoch 5/30\n",
            "1/1 [==============================] - 1s 509ms/step - loss: 0.6485 - accuracy: 0.6094\n",
            "Epoch 6/30\n",
            "1/1 [==============================] - 1s 520ms/step - loss: 0.6755 - accuracy: 0.6094\n",
            "Epoch 7/30\n",
            "1/1 [==============================] - 1s 501ms/step - loss: 0.6799 - accuracy: 0.5781\n",
            "Epoch 8/30\n",
            "1/1 [==============================] - 0s 496ms/step - loss: 0.7534 - accuracy: 0.4375\n",
            "Epoch 9/30\n",
            "1/1 [==============================] - 1s 509ms/step - loss: 0.6922 - accuracy: 0.5312\n",
            "Epoch 10/30\n",
            "1/1 [==============================] - 0s 465ms/step - loss: 0.6795 - accuracy: 0.6406\n",
            "Epoch 11/30\n",
            "1/1 [==============================] - 0s 492ms/step - loss: 0.6889 - accuracy: 0.5156\n",
            "Epoch 12/30\n",
            "1/1 [==============================] - 0s 486ms/step - loss: 0.6642 - accuracy: 0.5938\n",
            "Epoch 13/30\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 0.6502 - accuracy: 0.6250\n",
            "Epoch 14/30\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 0.6858 - accuracy: 0.5469\n",
            "Epoch 15/30\n",
            "1/1 [==============================] - 0s 494ms/step - loss: 0.6823 - accuracy: 0.5625\n",
            "Epoch 16/30\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 0.6586 - accuracy: 0.6562\n",
            "Epoch 17/30\n",
            "1/1 [==============================] - 0s 476ms/step - loss: 0.6575 - accuracy: 0.6094\n",
            "Epoch 18/30\n",
            "1/1 [==============================] - 1s 510ms/step - loss: 0.6425 - accuracy: 0.6094\n",
            "Epoch 19/30\n",
            "1/1 [==============================] - 0s 490ms/step - loss: 0.6764 - accuracy: 0.5938\n",
            "Epoch 20/30\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 0.7787 - accuracy: 0.4531\n",
            "Epoch 21/30\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 0.6591 - accuracy: 0.5781\n",
            "Epoch 22/30\n",
            "1/1 [==============================] - 0s 466ms/step - loss: 0.6489 - accuracy: 0.5938\n",
            "Epoch 23/30\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 0.6360 - accuracy: 0.5938\n",
            "Epoch 24/30\n",
            "1/1 [==============================] - 0s 472ms/step - loss: 0.6251 - accuracy: 0.6875\n",
            "Epoch 25/30\n",
            "1/1 [==============================] - 0s 494ms/step - loss: 0.7282 - accuracy: 0.5938\n",
            "Epoch 26/30\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 0.6668 - accuracy: 0.5469\n",
            "Epoch 27/30\n",
            "1/1 [==============================] - 0s 424ms/step - loss: 0.6697 - accuracy: 0.6094\n",
            "Epoch 28/30\n",
            "1/1 [==============================] - 0s 431ms/step - loss: 0.6606 - accuracy: 0.6562\n",
            "Epoch 29/30\n",
            "1/1 [==============================] - 0s 429ms/step - loss: 0.6501 - accuracy: 0.6562\n",
            "Epoch 30/30\n",
            "1/1 [==============================] - 0s 428ms/step - loss: 0.6280 - accuracy: 0.7344\n",
            "Epoch 1/30\n",
            "1/1 [==============================] - 1s 936ms/step - loss: 0.8636 - accuracy: 0.2656\n",
            "Epoch 2/30\n",
            "1/1 [==============================] - 0s 430ms/step - loss: 0.6079 - accuracy: 0.7031\n",
            "Epoch 3/30\n",
            "1/1 [==============================] - 0s 431ms/step - loss: 0.5812 - accuracy: 0.7500\n",
            "Epoch 4/30\n",
            "1/1 [==============================] - 0s 445ms/step - loss: 0.5628 - accuracy: 0.7500\n",
            "Epoch 5/30\n",
            "1/1 [==============================] - 1s 539ms/step - loss: 0.5295 - accuracy: 0.7500\n",
            "Epoch 6/30\n",
            "1/1 [==============================] - 1s 500ms/step - loss: 0.5200 - accuracy: 0.7656\n",
            "Epoch 7/30\n",
            "1/1 [==============================] - 1s 506ms/step - loss: 0.5355 - accuracy: 0.7656\n",
            "Epoch 8/30\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 0.5498 - accuracy: 0.7656\n",
            "Epoch 9/30\n",
            "1/1 [==============================] - 1s 508ms/step - loss: 0.6289 - accuracy: 0.7500\n",
            "Epoch 10/30\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 0.5251 - accuracy: 0.7500\n",
            "Epoch 11/30\n",
            "1/1 [==============================] - 1s 506ms/step - loss: 0.5254 - accuracy: 0.7812\n",
            "Epoch 12/30\n",
            "1/1 [==============================] - 1s 501ms/step - loss: 0.5604 - accuracy: 0.7500\n",
            "Epoch 13/30\n",
            "1/1 [==============================] - 0s 476ms/step - loss: 0.5183 - accuracy: 0.7656\n",
            "Epoch 14/30\n",
            "1/1 [==============================] - 0s 494ms/step - loss: 0.4840 - accuracy: 0.7969\n",
            "Epoch 15/30\n",
            "1/1 [==============================] - 0s 478ms/step - loss: 0.5568 - accuracy: 0.7500\n",
            "Epoch 16/30\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 0.4942 - accuracy: 0.7969\n",
            "Epoch 17/30\n",
            "1/1 [==============================] - 0s 486ms/step - loss: 0.5362 - accuracy: 0.7500\n",
            "Epoch 18/30\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 0.5438 - accuracy: 0.7344\n",
            "Epoch 19/30\n",
            "1/1 [==============================] - 0s 486ms/step - loss: 0.5683 - accuracy: 0.7500\n",
            "Epoch 20/30\n",
            "1/1 [==============================] - 0s 475ms/step - loss: 0.5244 - accuracy: 0.7500\n",
            "Epoch 21/30\n",
            "1/1 [==============================] - 0s 492ms/step - loss: 0.5161 - accuracy: 0.7500\n",
            "Epoch 22/30\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 0.4910 - accuracy: 0.7656\n",
            "Epoch 23/30\n",
            "1/1 [==============================] - 1s 504ms/step - loss: 0.4649 - accuracy: 0.7969\n",
            "Epoch 24/30\n",
            "1/1 [==============================] - 0s 487ms/step - loss: 0.4121 - accuracy: 0.7969\n",
            "Epoch 25/30\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 0.4927 - accuracy: 0.7812\n",
            "Epoch 26/30\n",
            "1/1 [==============================] - 0s 491ms/step - loss: 0.5479 - accuracy: 0.7656\n",
            "Epoch 27/30\n",
            "1/1 [==============================] - 0s 490ms/step - loss: 0.5156 - accuracy: 0.7969\n",
            "Epoch 28/30\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 0.4434 - accuracy: 0.7969\n",
            "Epoch 29/30\n",
            "1/1 [==============================] - 0s 488ms/step - loss: 0.4742 - accuracy: 0.7500\n",
            "Epoch 30/30\n",
            "1/1 [==============================] - 0s 434ms/step - loss: 0.6681 - accuracy: 0.6406\n",
            "Epoch 1/30\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.2069 - accuracy: 0.3600\n",
            "Epoch 2/30\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.7097 - accuracy: 0.5200\n",
            "Epoch 3/30\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.6840 - accuracy: 0.5200\n",
            "Epoch 4/30\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.6728 - accuracy: 0.6000\n",
            "Epoch 5/30\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.6870 - accuracy: 0.6400\n",
            "Epoch 6/30\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.6504 - accuracy: 0.6800\n",
            "Epoch 7/30\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.6785 - accuracy: 0.6800\n",
            "Epoch 8/30\n",
            "1/1 [==============================] - 0s 204ms/step - loss: 0.6687 - accuracy: 0.6800\n",
            "Epoch 9/30\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 0.6447 - accuracy: 0.6800\n",
            "Epoch 10/30\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 0.6595 - accuracy: 0.6800\n",
            "Epoch 11/30\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 0.6519 - accuracy: 0.6800\n",
            "Epoch 12/30\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.6497 - accuracy: 0.6800\n",
            "Epoch 13/30\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 0.6487 - accuracy: 0.6800\n",
            "Epoch 14/30\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.6392 - accuracy: 0.6800\n",
            "Epoch 15/30\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 0.6303 - accuracy: 0.6800\n",
            "Epoch 16/30\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 0.6354 - accuracy: 0.6800\n",
            "Epoch 17/30\n",
            "1/1 [==============================] - 0s 199ms/step - loss: 0.6163 - accuracy: 0.6800\n",
            "Epoch 18/30\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 0.6169 - accuracy: 0.6800\n",
            "Epoch 19/30\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.6215 - accuracy: 0.6800\n",
            "Epoch 20/30\n",
            "1/1 [==============================] - 0s 200ms/step - loss: 0.5987 - accuracy: 0.6800\n",
            "Epoch 21/30\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.5703 - accuracy: 0.6800\n",
            "Epoch 22/30\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 0.5870 - accuracy: 0.6800\n",
            "Epoch 23/30\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.5758 - accuracy: 0.6800\n",
            "Epoch 24/30\n",
            "1/1 [==============================] - 0s 200ms/step - loss: 0.6487 - accuracy: 0.6000\n",
            "Epoch 25/30\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.5836 - accuracy: 0.7600\n",
            "Epoch 26/30\n",
            "1/1 [==============================] - 0s 200ms/step - loss: 0.5780 - accuracy: 0.6800\n",
            "Epoch 27/30\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 0.6103 - accuracy: 0.6800\n",
            "Epoch 28/30\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 0.5645 - accuracy: 0.6800\n",
            "Epoch 29/30\n",
            "1/1 [==============================] - 0s 197ms/step - loss: 0.5724 - accuracy: 0.6800\n",
            "Epoch 30/30\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.5262 - accuracy: 0.6800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2Z87ndxKVhc"
      },
      "source": [
        "# Best Model Load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKlnkB9-ZFHh"
      },
      "source": [
        "#Req libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import cv2 \n",
        "import os\n",
        "\n",
        "from skimage import transform\n",
        "import skimage\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from keras.layers import LSTM\n",
        "from keras.models import Model \n",
        "from keras.layers import Input,Dense,TimeDistributed\n",
        "\n",
        "from keras.layers import Dropout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmaHbiBvZF3C"
      },
      "source": [
        "frame , row, col =(99,144,256)\n",
        "row_hidden = 128\n",
        "col_hidden = 128\n",
        "batch_size = 64    \n",
        "num_classes = 2\n",
        "epochs = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aC0Uhs3RKrm-"
      },
      "source": [
        "path = \"/content/drive/MyDrive/Minor Project 3/Checkpoints/Success 2/model_64[1].h5\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MCjC-W2KZ4U"
      },
      "source": [
        "x =Input(shape=(frame, row, col))\n",
        "encoded_rows = TimeDistributed(LSTM(row_hidden))(x) \n",
        "encoded_columns =LSTM(col_hidden)(encoded_rows)\n",
        "\n",
        "layer2 = Dense(128, activation='relu')(encoded_columns)\n",
        "layer3 = Dropout(.2)(layer2)\n",
        "layer4 = Dense(64, activation='relu')(layer3)\n",
        "layer5 = Dropout(.2)(layer4)\n",
        "prediction = Dense(num_classes, activation='softmax')(layer5)\n",
        "\n",
        "model = Model(x, prediction)\n",
        "model.load_weights(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RvbFUAZLozt",
        "outputId": "3ed983d5-eb81-4d78-8a8b-a15872327280"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='NAdam', metrics=['accuracy']) \n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 99, 144, 256)]    0         \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 99, 128)           197120    \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 2)                 130       \n",
            "=================================================================\n",
            "Total params: 353,602\n",
            "Trainable params: 353,602\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VYaJvkAMuos"
      },
      "source": [
        "os.chdir(\"/content/drive/MyDrive/Minor Project 3/Model 64/\")\n",
        "x_test_zero = np.load('x_test_batch[0].npy')\n",
        "x_test_one = np.load('x_test_batch[1].npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hn_pA21FOom3"
      },
      "source": [
        "x_test = np.concatenate((x_test_zero, x_test_one), axis = 0)\n",
        "y_test = np.load('y_test_np.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PsQpzXFNq3o",
        "outputId": "39786099-054e-494b-c21d-e32f6bdb666b"
      },
      "source": [
        "print('x_test_zero size = ', x_test_zero.shape)\n",
        "print('x_test_one size = ', x_test_one.shape)\n",
        "print('x_test size = ', x_test.shape)\n",
        "print('y_test size = ', y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_test_zero size =  (64, 99, 144, 256)\n",
            "x_test_one size =  (64, 99, 144, 256)\n",
            "x_test size =  (128, 99, 144, 256)\n",
            "y_test size =  (128, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHGFy1NjV7Wn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1c8111b-4faa-43d1-ecce-e0e87354c298"
      },
      "source": [
        "scores = model.evaluate(x_test, y_test, verbose=0) \n",
        "print('Test loss:', scores[0]) \n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.127873\n",
            "Test accuracy: 0.920457\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKP1fbJfJvhH"
      },
      "source": [
        "# Alert System"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmCVK72AZ6-5"
      },
      "source": [
        "def send_alertt(flag, camera_number):\n",
        "  \n",
        "  print('Prediction Probabilities are\\nNegative = ', flag[0][0],'\\nPositive = ',flag[0][1])\n",
        "\n",
        "  if(flag[0][1] > 0.6):\n",
        "    print('Accident Detected\\n')\n",
        "    send_alert(camera_number)\n",
        "  else:\n",
        "    print('No accident')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vSHIehsEv01"
      },
      "source": [
        "import pandas as pd\n",
        "import smtplib\n",
        "import os\n",
        "\n",
        "\n",
        "def send_alert(camera_number):\n",
        "  your_email = 'minorproject888@gmail.com'\n",
        "  your_password = 'dssvouztedbhbrjt'\n",
        "\n",
        "  server = smtplib.SMTP_SSL('smtp.gmail.com', 465)\n",
        "  server.ehlo()\n",
        "  server.login(your_email, your_password)\n",
        "\n",
        "\n",
        "  email_list = pd.read_excel(\"/content/drive/MyDrive/Minor Project 3/Email/EmailL.xlsx\")\n",
        "  camera = pd.read_excel(\"/content/drive/MyDrive/Minor Project 3/Email/Cameras.xlsx\")\n",
        "\n",
        "  gps_location = camera.loc[camera_number]\n",
        "\n",
        "  all_emails = email_list['Email']\n",
        "\n",
        "  for idx in range(len(all_emails)):\n",
        "\n",
        "      email = all_emails[idx]\n",
        "      subject = 'Requesting Immediate Help'\n",
        "      message = 'Urgent - An accident has occurred.\\nLocation -> ' + str(gps_location['Location']) + '\\nGoogle Maps Link -> ' + str(gps_location['GPS Link'] + \n",
        "                                                                                                                                    '\\n\\nNOTE : This is not an actual accident, only an emulation of an accident\\n')\n",
        "     \n",
        "      full_email = (\"From: <{0}>\\n\"\n",
        "                    \"To:  <{1}>\\n\"\n",
        "                    \"Subject: {2}\\n\\n\"\n",
        "                    \"{3}\"\n",
        "                    .format(your_email, email, subject, message))\n",
        "\n",
        "      try:\n",
        "          server.sendmail(your_email, [email], full_email)\n",
        "          print('Email to {} successfully sent!\\n'.format(email))\n",
        "      except Exception as e:\n",
        "          print('Email to {} could not be sent :( because {}\\n\\n'.format(email, str(e)))\n",
        "\n",
        "  server.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ur5lqwmzfizI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e6bd3a0-5cf4-4eb9-8f8c-f4c30a2bc5ff"
      },
      "source": [
        "path = \"/content/drive/MyDrive/Minor Project 3/Run Cases/Positive/\"\n",
        "#path = \"/content/drive/MyDrive/Minor Project 3/Run Cases/Negative/\"\n",
        "\n",
        "video_name = '03.mp4'\n",
        "\n",
        "camera_number = 2\n",
        "\n",
        "\n",
        "cap = cv2.VideoCapture(path+video_name)\n",
        "property_id = int(cv2.CAP_PROP_FRAME_COUNT) \n",
        "length = int(cv2.VideoCapture.get(cap, property_id))\n",
        "count = 0\n",
        "success = 1\n",
        "\n",
        "\n",
        "if length > 100:\n",
        "  cut = length-99\n",
        "else:\n",
        "  cut = 0\n",
        "\n",
        "video_ready = []\n",
        "\n",
        "for i in range(length):\n",
        "  success, image = cap.read()\n",
        "  \n",
        "  if count >= cut :\n",
        "    n = count-cut\n",
        "    tmp = skimage.color.rgb2gray(np.array(image))\n",
        "    tmp = transform.resize(tmp, (144, 256))\n",
        "    video_ready.append(tmp)\n",
        "  count += 1\n",
        "\n",
        "\n",
        "\n",
        "video_ready = [video_ready]\n",
        "video_ready = np.array(video_ready)\n",
        "\n",
        "video_ready.shape\n",
        "\n",
        "flag = model.predict(video_ready)\n",
        "\n",
        "send_alertt(flag, camera_number)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction Probabilities are\n",
            "Negative =  0.10617728 \n",
            "Positive =  0.8938228\n",
            "Accident Detected\n",
            "\n",
            "Email to gurpreet.e9842@cumail.in successfully sent!\n",
            "\n",
            "Email to 18bcs6124@cuchd.in successfully sent!\n",
            "\n",
            "Email to 18bcs6129@cuchd.in successfully sent!\n",
            "\n",
            "Email to 18bcs6131@cuchd.in successfully sent!\n",
            "\n",
            "Email to 18bcs6138@cuchd.in successfully sent!\n",
            "\n",
            "Email to 18bcs6145@cuchd.in successfully sent!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBmw5QjfmXFS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}